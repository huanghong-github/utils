{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from path import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "\n",
    "from sklearn import preprocessing \n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch.transforms import ToTensorV2\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root=Path(r'')\n",
    "imagedir=root/'JPEGImages'\n",
    "xmldir=root/'Annotations'\n",
    "\n",
    "class XmlParser(object):\n",
    "    \n",
    "    def __init__(self,xml_file):\n",
    "\n",
    "        self.xml_file = xml_file\n",
    "        self._root = ET.parse(self.xml_file).getroot()\n",
    "        self._objects = self._root.findall(\"object\")\n",
    "        # path to the image file as describe in the xml file\n",
    "        self.img_path = imagedir/self._root.find('filename').text\n",
    "        # image id \n",
    "        self.image_id = self.img_path.stem\n",
    "        # names of the classes contained in the xml file\n",
    "        self.names = self._get_names()\n",
    "        # coordinates of the bounding boxes\n",
    "        self.boxes = self._get_bndbox()\n",
    "\n",
    "    def parse_xml(self):\n",
    "        \"\"\"\"Parse the xml file returning the root.\"\"\"\n",
    "    \n",
    "        tree = ET.parse(self.xml_file)\n",
    "        return tree.getroot()\n",
    "\n",
    "    def _get_names(self):\n",
    "        \n",
    "        names = [obj.find(\"name\").text for obj in self._objects]\n",
    "        return np.array(names)\n",
    "\n",
    "    def _get_bndbox(self):\n",
    "\n",
    "        boxes = []\n",
    "        for obj in self._objects:\n",
    "            coordinates = []\n",
    "            bndbox = obj.find(\"bndbox\")\n",
    "            coordinates.append(np.float32(bndbox.find(\"xmin\").text))\n",
    "            coordinates.append(np.float32(bndbox.find(\"ymin\").text))\n",
    "            coordinates.append(np.float32(bndbox.find(\"xmax\").text))\n",
    "            coordinates.append(np.float32(bndbox.find(\"ymax\").text))\n",
    "            boxes.append(coordinates)\n",
    "\n",
    "        return np.array(boxes)\n",
    "\n",
    "\n",
    "def xml_files_to_df(xml_files):\n",
    "        \n",
    "    \"\"\"\"Return pandas dataframe from list of XML files.\"\"\"\n",
    "    \n",
    "    names = []\n",
    "    boxes = []\n",
    "    image_id = []\n",
    "    xml_path = []\n",
    "    img_path = []\n",
    "    for file in xml_files:\n",
    "        xml = XmlParser(file)\n",
    "        names.extend(xml.names)\n",
    "        boxes.extend(xml.boxes)\n",
    "        image_id.extend([xml.image_id] * len(xml.names))\n",
    "        xml_path.extend([xml.xml_file] * len(xml.names))\n",
    "        img_path.extend([xml.img_path] * len(xml.names))\n",
    "        \n",
    "    data = {\"image_id\": image_id,\n",
    "            \"names\": names,\n",
    "            \"boxes\": boxes,\n",
    "            \"xml_path\":xml_path,\n",
    "            \"img_path\":img_path}\n",
    "    \n",
    "    df = pd.DataFrame.from_dict(data, orient='index')\n",
    "    df = df.transpose()\n",
    "\n",
    "    # df['xmin'] = -1\n",
    "    # df['ymin'] = -1\n",
    "    # df['xmax'] = -1\n",
    "    # df['ymax'] = -1\n",
    "\n",
    "    df[['xmin','ymin','xmax','ymax']]=np.stack(df['boxes'][i] for i in range(len(df['boxes'])))\n",
    "\n",
    "    # df.drop(columns=['boxes'], inplace=True)\n",
    "    # df['xmin'] = df['xmin'].astype(np.float32)\n",
    "    # df['ymin'] = df['ymin'].astype(np.float32)\n",
    "    # df['xmax'] = df['xmax'].astype(np.float32)\n",
    "    # df['ymax'] = df['ymax'].astype(np.float32)\n",
    "\n",
    "    enc = preprocessing.LabelEncoder()\n",
    "    df['labels'] = enc.fit_transform(df['names'])\n",
    "    df['labels'] = np.stack(df['labels'][i]+1 for i in range(len(df['labels']))) \n",
    "    return df\n",
    "\n",
    "df = xml_files_to_df(xmldir.files())\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = df[['names','labels']].value_counts()\n",
    "print(classes)\n",
    "classes = {item.labels:item.names for item in classes.reset_index().itertuples(index=False) }\n",
    "print(classes)\n",
    "\n",
    "image_ids = df['image_id'].unique()\n",
    "train_len = int(len(image_ids)*0.8)\n",
    "train_ids = image_ids[:train_len]\n",
    "valid_ids = image_ids[train_len:]\n",
    "\n",
    "valid_df = df[df['image_id'].isin(valid_ids)]\n",
    "train_df = df[df['image_id'].isin(train_ids)]\n",
    "print(train_df.shape, valid_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VOCDataset(Dataset):\n",
    "        \n",
    "    def __init__(self, dataframe, image_dir, transforms=None):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.image_ids = dataframe['image_id'].unique()\n",
    "        self.df = dataframe\n",
    "        self.image_dir = image_dir\n",
    "        self.transforms = transforms\n",
    "    \n",
    "    def _parse(self, index: int):\n",
    "        image_id = self.image_ids[index]\n",
    "        records = self.df[self.df['image_id'] == image_id]\n",
    "        \n",
    "        image = cv2.imread(f'{self.image_dir}/{image_id}.jpg', cv2.IMREAD_COLOR)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
    "        image /= 255.0\n",
    "        \n",
    "        # boxes = records[['xmin', 'ymin', 'xmax', 'ymax']].values\n",
    "        boxes=np.stack(records['boxes'].values)\n",
    "       \n",
    "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
    "        area = torch.as_tensor(area, dtype=torch.float32)\n",
    "        \n",
    "        label = records['labels'].values\n",
    "        labels = torch.as_tensor(label, dtype=torch.int64)\n",
    "        \n",
    "        # suppose all instances are not crowd\n",
    "        iscrowd = torch.zeros((records.shape[0],), dtype=torch.int64)\n",
    "        \n",
    "        target = {\n",
    "            'boxes': boxes,\n",
    "            'labels': labels,\n",
    "            'image_id': torch.tensor([index]),\n",
    "            'area': area,\n",
    "            'iscrowd': iscrowd\n",
    "        }\n",
    "        return image, target \n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        image, target = self._parse(index)\n",
    "        \n",
    "        if self.transforms:\n",
    "            sample = {\n",
    "                'image': image,\n",
    "                'bboxes': target['boxes'],\n",
    "                'labels': target['labels']\n",
    "            }\n",
    "            sample = self.transforms(**sample)\n",
    "            image = sample['image']\n",
    "            \n",
    "            # target['boxes'] = torch.stack(tuple(map(torch.tensor, zip(*sample['bboxes'])))).permute(1,0)\n",
    "            target['boxes'] = torch.stack(tuple(map(torch.tensor, sample['bboxes'])))\n",
    "            \n",
    "        return image, target\n",
    "\n",
    "    def coco_index(self, index):\n",
    "        \"\"\"\n",
    "        该方法是专门为pycocotools统计标签信息准备，不对图像和标签作任何处理\n",
    "        由于不用去读取图片，可大幅缩减统计时间\n",
    "        \"\"\"\n",
    "        # read xml\n",
    "        image, target = self._parse(index)\n",
    "        data_height,data_width = image.shape[:2]\n",
    "        return (data_height, data_width), target\n",
    "        \n",
    "    def __len__(self) -> int:\n",
    "        return self.image_ids.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transform_train():\n",
    "    return A.Compose([\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.RandomBrightnessContrast(p=0.2),\n",
    "        A.Normalize(max_pixel_value=1),\n",
    "        ToTensorV2(p=1.0)\n",
    "    ], bbox_params={'format':'pascal_voc', 'label_fields': ['labels']})\n",
    "\n",
    "def get_transform_valid():\n",
    "    return A.Compose([\n",
    "        A.Normalize(max_pixel_value=1),\n",
    "        ToTensorV2(p=1.0)\n",
    "    ], bbox_params={'format': 'pascal_voc', 'label_fields':['labels']})\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "train_dataset = VOCDataset(train_df, imagedir, get_transform_train())\n",
    "valid_dataset = VOCDataset(valid_df, imagedir, get_transform_valid())\n",
    "\n",
    "# split the dataset in train and test set\n",
    "# indices = torch.randperm(len(train_dataset)).tolist()\n",
    "train_data_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=4,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "valid_data_loader = DataLoader(\n",
    "    valid_dataset,\n",
    "    batch_size=4,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    "    collate_fn=collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def denorm(x):\n",
    "    mean=(0.485, 0.456, 0.406)\n",
    "    std=(0.229, 0.224, 0.225)\n",
    "    mean=torch.Tensor(mean)\n",
    "    std=torch.Tensor(std)\n",
    "    area=x.shape[1]*x.shape[2]\n",
    "    mean=mean.reshape(3,1).repeat(1,area).reshape(x.shape)\n",
    "    std = std.reshape(3,1).repeat(1,area).reshape(x.shape)\n",
    "    out = x.mul(std).add(mean).clamp(0,1) # 逆正则化  Normalize(mean=[0.5], std=[0.5])]  (x-0.5)/0.5=2x-1\n",
    "    out = out.mul(255).add(0.5).clamp(0, 255) # 恢复至区间[0,255]\n",
    "    return out.permute(1,2,0).to(\"cpu\", torch.uint8).numpy() # 改变维度，转成整数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "images, targets= next(iter(train_data_loader))\n",
    "images = list(image.to(device) for image in images)\n",
    "targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "plt.figure(figsize=(20,20))\n",
    "for i, (image, target) in enumerate(zip(images, targets)):\n",
    "    plt.subplot(2,2, i+1)\n",
    "    boxes = targets[i]['boxes'].cpu().numpy().astype(np.int32)\n",
    "    sample = denorm(images[i])\n",
    "    # sample = images[i].permute(1,2,0).cpu().numpy()\n",
    "    names = targets[i]['labels'].cpu().numpy().astype(np.int64)\n",
    "    for i,box in enumerate(boxes):\n",
    "        cv2.rectangle(sample,\n",
    "                      (box[0], box[1]),\n",
    "                      (box[2], box[3]),\n",
    "                      (0, 0, 220), 2)\n",
    "        cv2.putText(sample, classes[names[i]], (box[0],box[1]+15),cv2.FONT_HERSHEY_COMPLEX ,0.5,(0,220,0),1,cv2.LINE_AA)  \n",
    "\n",
    "    plt.axis('off')\n",
    "    plt.imshow(sample)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FasterRCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torch\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection import FasterRCNN\n",
    "from torchvision.models.detection.rpn import AnchorGenerator\n",
    "\n",
    "num_classes = 4\n",
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True, num_classes = num_classes)\n",
    "\n",
    "# get number of input features for the classifier\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "\n",
    "# replace the pre-trained head with a new one\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr=0.01, weight_decay=0.0001)\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)\n",
    "\n",
    "from train_eval_utils import train_one_epoch,evaluate\n",
    "num_epochs = 100\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(epoch)\n",
    "    # train for one epoch, printing every 10 iterations\n",
    "    train_one_epoch(model, optimizer, train_data_loader, device, epoch, print_freq=10)\n",
    "    # update the learning rate\n",
    "    lr_scheduler.step()\n",
    "    # evaluate on the test dataset\n",
    "    evaluate(model, valid_data_loader, device=device)\n",
    "    save_files = {\n",
    "            'model': model.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'lr_scheduler': lr_scheduler.state_dict(),\n",
    "            'epoch': epoch}\n",
    "    if (epoch+1)%25==0:       \n",
    "        torch.save(save_files, f'/project/train/models/faster_rcnn_state_{epoch+1}.pth')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

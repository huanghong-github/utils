{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "\n",
    "from sklearn import preprocessing \n",
    "\n",
    "\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch.transforms import ToTensorV2\n",
    "\n",
    "import torch\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import path\n",
    "imagedir=path.Path('/home/data/831')\n",
    "xmldir=path.Path('/home/data/831')\n",
    "images=[f for f in imagedir.files() if f.endswith('jpg')]\n",
    "xmls=[f for f in xmldir.files() if f.endswith('xml')]\n",
    "\n",
    "\n",
    "class XmlParser(object):\n",
    "    \n",
    "    def __init__(self,xml_file):\n",
    "\n",
    "        self.xml_file = xml_file\n",
    "        self._root = ET.parse(self.xml_file).getroot()\n",
    "        self._objects = self._root.findall(\"object\")\n",
    "        # path to the image file as describe in the xml file\n",
    "        self.img_path = imagedir/self._root.find('filename').text\n",
    "        # image id \n",
    "        self.img_id = self.img_path.stem\n",
    "        # names of the classes contained in the xml file\n",
    "        self.names = self._get_names()\n",
    "        # coordinates of the bounding boxes\n",
    "        self.boxes = self._get_bndbox()\n",
    "\n",
    "    def parse_xml(self):\n",
    "        \"\"\"\"Parse the xml file returning the root.\"\"\"\n",
    "    \n",
    "        tree = ET.parse(self.xml_file)\n",
    "        return tree.getroot()\n",
    "\n",
    "    def _get_names(self):\n",
    "\n",
    "        names = []\n",
    "        for obj in self._objects:\n",
    "            name = obj.find(\"name\")\n",
    "            names.append(name.text)\n",
    "\n",
    "        return np.array(names)\n",
    "\n",
    "    def _get_bndbox(self):\n",
    "\n",
    "        boxes = []\n",
    "        for obj in self._objects:\n",
    "            coordinates = []\n",
    "            bndbox = obj.find(\"bndbox\")\n",
    "            coordinates.append(np.int32(np.float32(bndbox.find(\"xmin\").text)))\n",
    "            coordinates.append(np.int32(np.float32(bndbox.find(\"ymin\").text)))\n",
    "            coordinates.append(np.int32(np.float32(bndbox.find(\"xmax\").text)))\n",
    "            coordinates.append(np.int32(np.float32(bndbox.find(\"ymax\").text)))\n",
    "            boxes.append(coordinates)\n",
    "\n",
    "        return np.array(boxes)\n",
    "\n",
    "\n",
    "def xml_files_to_df(xml_files):\n",
    "        \n",
    "    \"\"\"\"Return pandas dataframe from list of XML files.\"\"\"\n",
    "    \n",
    "    names = []\n",
    "    boxes = []\n",
    "    img_id = []\n",
    "    xml_path = []\n",
    "    img_path = []\n",
    "    for file in xml_files:\n",
    "        xml = XmlParser(file)\n",
    "        names.extend(xml.names)\n",
    "        boxes.extend(xml.boxes)\n",
    "        img_id.extend([xml.img_id] * len(xml.names))\n",
    "        xml_path.extend([xml.xml_file] * len(xml.names))\n",
    "        img_path.extend([xml.img_path] * len(xml.names))\n",
    "    a = {\"img_id\": img_id,\n",
    "         \"names\": names,\n",
    "         \"boxes\": boxes,\n",
    "         \"xml_path\":xml_path,\n",
    "         \"img_path\":img_path}\n",
    "    \n",
    "    df = pd.DataFrame.from_dict(a, orient='index')\n",
    "    df = df.transpose()\n",
    "    df['xmin'] = -1\n",
    "    df['ymin'] = -1\n",
    "    df['xmax'] = -1\n",
    "    df['ymax'] = -1\n",
    "\n",
    "    df[['xmin','ymin','xmax','ymax']]=np.stack(df['boxes'][i] for i in range(len(df['boxes'])))\n",
    "\n",
    "    df.drop(columns=['boxes'], inplace=True)\n",
    "    df['xmin'] = df['xmin'].astype(np.float32)\n",
    "    df['ymin'] = df['ymin'].astype(np.float32)\n",
    "    df['xmax'] = df['xmax'].astype(np.float32)\n",
    "    df['ymax'] = df['ymax'].astype(np.float32)\n",
    "    return df\n",
    "\n",
    "df = xml_files_to_df(xmls)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = preprocessing.LabelEncoder()\n",
    "df['labels'] = enc.fit_transform(df['names'])\n",
    "df['labels'] = np.stack(df['labels'][i]+1 for i in range(len(df['labels']))) \n",
    "\n",
    "\n",
    "classes = df[['names','labels']].value_counts()\n",
    "classes = {item.labels:item.names for item in classes.reset_index().itertuples(index=False) }\n",
    "print(classes)\n",
    "\n",
    "\n",
    "image_ids = df['img_id'].unique()\n",
    "print(len(image_ids))\n",
    "train_ids = image_ids[:80]\n",
    "valid_ids = image_ids[80:]\n",
    "\n",
    "\n",
    "valid_df = df[df['img_id'].isin(valid_ids)]\n",
    "train_df = df[df['img_id'].isin(train_ids)]\n",
    "print(valid_df.shape, train_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VOCDataset(Dataset):\n",
    "        \n",
    "    def __init__(self, dataframe, image_dir, transforms=None):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.image_ids = dataframe['img_id'].unique()\n",
    "        self.df = dataframe\n",
    "        self.image_dir = image_dir\n",
    "        self.transforms = transforms\n",
    "    \n",
    "    def __getitem__(self, index: int):\n",
    "        image_id = self.image_ids[index]\n",
    "        records = self.df[self.df['img_id'] == image_id]\n",
    "        \n",
    "        image = cv2.imread(f'{self.image_dir}/{image_id}.jpg', cv2.IMREAD_COLOR)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
    "        image /= 255.0\n",
    "        rows, cols = image.shape[:2]\n",
    "        \n",
    "        boxes = records[['xmin', 'ymin', 'xmax', 'ymax']].values\n",
    "        \n",
    "       \n",
    "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
    "        area = torch.as_tensor(area, dtype=torch.float32)\n",
    "        \n",
    "        label = records['labels'].values\n",
    "        labels = torch.as_tensor(label, dtype=torch.int64)\n",
    "        \n",
    "        # suppose all instances are not crowd\n",
    "        iscrowd = torch.zeros((records.shape[0],), dtype=torch.int64)\n",
    "        \n",
    "        target = {}\n",
    "        target['boxes'] = boxes\n",
    "        target['labels'] = labels\n",
    "        # target['masks'] = None\n",
    "        target['image_id'] = torch.tensor([index])\n",
    "        target['area'] = area\n",
    "        target['iscrowd'] = iscrowd\n",
    "        \n",
    "        if self.transforms:\n",
    "            sample = {\n",
    "                'image': image,\n",
    "                'bboxes': target['boxes'],\n",
    "                'labels': labels\n",
    "            }\n",
    "            sample = self.transforms(**sample)\n",
    "            image = sample['image']\n",
    "            \n",
    "            target['boxes'] = torch.stack(tuple(map(torch.tensor, zip(*sample['bboxes'])))).permute(1,0)\n",
    "            \n",
    "            return image, target\n",
    "        \n",
    "    def __len__(self) -> int:\n",
    "        return self.image_ids.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transform_train():\n",
    "    return A.Compose([\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.RandomBrightnessContrast(p=0.2),\n",
    "        ToTensorV2(p=1.0)\n",
    "    ], bbox_params={'format':'pascal_voc', 'label_fields': ['labels']})\n",
    "\n",
    "def get_transform_valid():\n",
    "    return A.Compose([\n",
    "        ToTensorV2(p=1.0)\n",
    "    ], bbox_params={'format': 'pascal_voc', 'label_fields':['labels']})\n",
    "\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "        return tuple(zip(*batch))\n",
    "\n",
    "train_dataset = VOCDataset(train_df, imagedir, get_transform_train())\n",
    "valid_dataset = VOCDataset(valid_df, imagedir, get_transform_valid())\n",
    "\n",
    "\n",
    "# split the dataset in train and test set\n",
    "# indices = torch.randperm(len(train_dataset)).tolist()\n",
    "\n",
    "\n",
    "train_data_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=4,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "valid_data_loader = DataLoader(\n",
    "    valid_dataset,\n",
    "    batch_size=4,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    "    collate_fn=collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "images, targets= next(iter(train_data_loader))\n",
    "images = list(image.to(device) for image in images)\n",
    "targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "plt.figure(figsize=(20,20))\n",
    "for i, (image, target) in enumerate(zip(images, targets)):\n",
    "    plt.subplot(2,2, i+1)\n",
    "    boxes = targets[i]['boxes'].cpu().numpy().astype(np.int32)\n",
    "    sample = images[i].permute(1,2,0).cpu().numpy()\n",
    "    names = targets[i]['labels'].cpu().numpy().astype(np.int64)\n",
    "    for i,box in enumerate(boxes):\n",
    "        cv2.rectangle(sample,\n",
    "                      (box[0], box[1]),\n",
    "                      (box[2], box[3]),\n",
    "                      (0, 0, 220), 2)\n",
    "        cv2.putText(sample, classes[names[i]], (box[0],box[1]+15),cv2.FONT_HERSHEY_COMPLEX ,0.5,(0,220,0),1,cv2.LINE_AA)  \n",
    "\n",
    "    plt.axis('off')\n",
    "    plt.imshow(sample)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
